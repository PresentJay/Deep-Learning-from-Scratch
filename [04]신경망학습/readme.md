# 신경망 학습

- 학습이란, 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것!
- 손실함수 : 신경망이 학습할 수 있도록 해주는 지표
- 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표!

<br>

        [데이터 주도 학습]
        * 데이터에서 답을 찾고, 패턴을 발견하는 것이 바로 기계학습!
        * 주어진 데이터의 특징(feature)을 추출하고, 그 패턴을 기계학습 기술로 학습
        * 특징(Feature)은 입력 데이터에서 중요한 데이터를 정확히 추출하도록 설계된 변환기

<br>

        [신경망(딥러닝)]
        * 신경망은 데이터를 있는 그대로 학습, 특징까지도 스스로 학습!
        * 종단간 기계학습(end to end machine learning)이라고도 함!

<br>

        [훈련 데이터와 시험 데이터]
        * 훈련 데이터(training data) : 학습용 데이터 - 최적의 매개변수를 찾음
        * 시험 데이터(test data) : 훈련한 모델의 실력을 평가 (범용 능력)
        * 오버피팅(overfitting) : 특정 데이터셋에만 성능을 보이도록 지나치게 최적화된 상태

<br>

        [손실함수(loss function)]
        * 신경망 성능의 '나쁨'을 나타내는 지표 (훈련 데이터를 처리하지 못하는 비율)
        * 일반적으로 오차제곱합과 교차 엔트로피 오차를 사용

<br>

        [오차제곱합(Sum of Squares for Error, SSE)]
        * E = (0.5) * [각 차원에서 (신경망 출력 - 정답 레이블)의 제곱들의 총합]

<br>

        [교차 엔트로피 오차(Cross Entropy Error, CEE)]
        * E = (-1) * [(k가 정답인 one-hot-coding 정답테이블) * (자연로그 : k를 정답으로 추론하는 테이블)]

<br>

        [미니배치(mini-batch)]
        * 훈련 데이터 전체에 대한 손실함수를 구하려면 매우 오래 걸릴 수 있음
        * 이런 경우 데이터 일부를 추려, 전체의 '근사치'로 이용하는 것을 미니배치라고 함

<br><br><br>

> #### [C.f.01.] 퍼셉트론에서 보였던 직선으로 분리가능한 문제는 유한 번의 학습을 통해 학습할 수 있다는 것이 증명됨 (Perceptron convergence theorem - 퍼셉트론 수렴 정리)

> #### [C.f.02.] 비선형 분리문제는 자동으로 학습할 수 없음

> #### [C.f.03.] 자연로그는 아래 그래프처럼 x가 1일때 y가 0이 되고, x가 0에 가까워질수록 y의 값이 점점 작아짐. 즉, 정답일 때의 출력이 작아질수록 오차가 커짐
>
> ![ ](./image/01.PNG)
